{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOwZ1w9teOhL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation of Simple Perceptron and Analysis of its Limitation on XOR Problem**\n",
        "This notebook is an Open Educational Resource (OER) developed for teaching and learning purposes. It is released under the Creative Commons Attributionâ€“ShareAlike (CC BY-SA 4.0) International License.\n",
        "\n",
        "This license allows anyone to use, copy, adapt, modify, translate, remix, and redistribute the material in any medium or format, provided proper credit is given to the original author and any modified versions are shared under the same license.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*Citation Format:*\n",
        " *Suneel Kumar Duvvuri, Implementation of Simple Perceptron and Analysis of its Limitation on XOR Problem. Open Educational Resource (OER). Licensed under CC BY-SA 4.0*"
      ],
      "metadata": {
        "id": "6JKWldqIfy-0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dff3f7b6"
      },
      "source": [
        "### 1. Perceptron Implementation\n",
        "\n",
        "First, let's implement a basic Perceptron model. A Perceptron is a single-layer neural network used for binary classification. It takes multiple binary inputs and produces a single binary output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18634b64"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                # Calculate weighted sum and apply activation function (step function)\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = 1 if linear_output >= 0 else 0\n",
        "\n",
        "                # Perceptron learning rule (update weights and bias if prediction is wrong)\n",
        "                update = self.learning_rate * (y[idx] - y_predicted)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return np.where(linear_output >= 0, 1, 0)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50cd3a6b"
      },
      "source": [
        "### 2. Perceptron Experiment: AND Gate\n",
        "\n",
        "Let's train our Perceptron to solve a simple **AND** gate. An AND gate is a linearly separable problem, meaning a single straight line can separate the true and false outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84be5313",
        "outputId": "945d68e9-5111-4d02-b0bc-9a8079fe96ba"
      },
      "source": [
        "# Input data for AND gate\n",
        "X_and = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Output data for AND gate\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Create and train the Perceptron\n",
        "perceptron_and = Perceptron(learning_rate=0.1, n_iterations=100)\n",
        "perceptron_and.fit(X_and, y_and)\n",
        "\n",
        "# Test the trained Perceptron\n",
        "predictions_and = perceptron_and.predict(X_and)\n",
        "\n",
        "print(\"AND Gate Predictions:\")\n",
        "for i in range(len(X_and)):\n",
        "    print(f\"Input: {X_and[i]}, Expected: {y_and[i]}, Predicted: {predictions_and[i]}\")\n",
        "\n",
        "accuracy_and = np.mean(predictions_and == y_and)\n",
        "print(f\"\\nAccuracy for AND Gate: {accuracy_and * 100:.2f}%\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate Predictions:\n",
            "Input: [0 0], Expected: 0, Predicted: 0\n",
            "Input: [0 1], Expected: 0, Predicted: 0\n",
            "Input: [1 0], Expected: 0, Predicted: 0\n",
            "Input: [1 1], Expected: 1, Predicted: 1\n",
            "\n",
            "Accuracy for AND Gate: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b82671cd"
      },
      "source": [
        "### 3. Perceptron Drawback: XOR Gate\n",
        "\n",
        "Now, let's try to train the Perceptron on an **XOR** gate. The XOR (exclusive OR) problem is a classic example of a non-linearly separable problem. This means no single straight line can separate the true and false outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6e3cc55",
        "outputId": "1abe401f-d2bc-4daa-e477-456e83cfc0f3"
      },
      "source": [
        "# Input data for XOR gate\n",
        "X_xor = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Output data for XOR gate\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Create and train the Perceptron\n",
        "perceptron_xor = Perceptron(learning_rate=0.1, n_iterations=100)\n",
        "perceptron_xor.fit(X_xor, y_xor)\n",
        "\n",
        "# Test the trained Perceptron\n",
        "predictions_xor = perceptron_xor.predict(X_xor)\n",
        "\n",
        "print(\"XOR Gate Predictions:\")\n",
        "for i in range(len(X_xor)):\n",
        "    print(f\"Input: {X_xor[i]}, Expected: {y_xor[i]}, Predicted: {predictions_xor[i]}\")\n",
        "\n",
        "accuracy_xor = np.mean(predictions_xor == y_xor)\n",
        "print(f\"\\nAccuracy for XOR Gate: {accuracy_xor * 100:.2f}%\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Gate Predictions:\n",
            "Input: [0 0], Expected: 0, Predicted: 1\n",
            "Input: [0 1], Expected: 1, Predicted: 1\n",
            "Input: [1 0], Expected: 1, Predicted: 0\n",
            "Input: [1 1], Expected: 0, Predicted: 0\n",
            "\n",
            "Accuracy for XOR Gate: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6394420d"
      },
      "source": [
        "### Explanation of Perceptron's Failure on XOR\n",
        "\n",
        "As you can see from the XOR gate results, the simple Perceptron fails to achieve 100% accuracy (or sometimes even close to it). This is because:\n",
        "\n",
        "*   **Linear Separability:** A Perceptron can only classify linearly separable data. This means if you can draw a single straight line (or hyperplane in higher dimensions) to separate the different classes, a Perceptron can learn it.\n",
        "\n",
        "*   **XOR's Non-Linear Nature:** The XOR function is not linearly separable. If you plot the inputs `(0,0)`, `(0,1)`, `(1,0)`, `(1,1)` and their corresponding outputs, you'll find that `(0,0)` and `(1,1)` are one class (0), and `(0,1)` and `(1,0)` are another class (1). No single straight line can separate these two groups of points.\n",
        "\n",
        "    *   For example, consider a 2D plane: points (0,0) and (1,1) have output 0, while (0,1) and (1,0) have output 1. Any line you draw will incorrectly classify at least one point.\n",
        "\n",
        "*   **Solution:** To solve non-linearly separable problems like XOR, a single-layer Perceptron is insufficient. You need a more complex architecture, such as a **multi-layer Perceptron (MLP)**, also known as a feedforward neural network with at least one hidden layer. Hidden layers allow the network to learn non-linear decision boundaries by transforming the input data into a new feature space where it becomes linearly separable."
      ]
    }
  ]
}